{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data_generation\"               \n",
    "FILE_PATTERN = \"quad_data_run_*.csv\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3 \n",
    "PRETRAIN_EPOCHS = 2000       # train data-only first\n",
    "PHYS_RAMP_EPOCHS = 2000      # ramp physics weight gradually\n",
    "TOTAL_EPOCHS = PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS + 4000\n",
    "PRINT_EVERY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics weights (base values; physics weight will be ramped)\n",
    "LAMBDA_BASE = 1e-4\n",
    "L_z = 1.0\n",
    "L_trans = 1.0\n",
    "L_rot = 1.0\n",
    "L_kin = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrotor constants (same as MATLAB)\n",
    "Jxx = 6.86e-5; Jyy = 9.2e-5; Jzz = 1.366e-4\n",
    "m = 0.068; g = 9.81\n",
    "t1 = (Jyy - Jzz) / Jxx\n",
    "t2 = (Jzz - Jxx) / Jyy\n",
    "t3 = (Jxx - Jyy) / Jzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_array(values, deg_to_rad=False, negate=False):\n",
    "    arr = np.array(values, dtype=np.float64)\n",
    "    \n",
    "    arr[np.isnan(arr)] = 0.0\n",
    "    arr[np.isinf(arr)] = 0.0\n",
    "\n",
    "    if deg_to_rad:\n",
    "        arr = arr * np.pi / 180.0\n",
    "    if negate:\n",
    "        arr = -arr\n",
    "\n",
    "    arr = np.clip(arr, -1e6, 1e6)\n",
    "\n",
    "    return arr.reshape(-1, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 files, total samples = 16000\n"
     ]
    }
   ],
   "source": [
    "csv_files = sorted(glob.glob(os.path.join(DATA_DIR, FILE_PATTERN)))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found with pattern {FILE_PATTERN} in {DATA_DIR}\")\n",
    "\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(csv_files)} files, total samples = {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No NaN values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "nan_counts = data.isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(nan_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing NaN values:\")\n",
    "    print(nan_cols)\n",
    "    print(\"\\nRows with NaN values:\")\n",
    "    print(data[data.isna().any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No NaN values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No Inf or -Inf values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "inf_mask = np.isinf(data.select_dtypes(include=[np.number])).any()\n",
    "inf_cols = inf_mask[inf_mask].index.tolist()\n",
    "\n",
    "if len(inf_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing Inf or -Inf values:\")\n",
    "    print(inf_cols)\n",
    "    print(\"\\nRows with Inf values:\")\n",
    "    print(data[np.isinf(data.select_dtypes(include=[np.number])).any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No Inf or -Inf values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 11200\n",
      "Test samples:  4800\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Select columns + preprocessing\n",
    "# -------------------------\n",
    "# Column names as generated by MATLAB code earlier\n",
    "# Time_s,Height_m,X_m,Y_m,Roll_deg,Pitch_deg,Yaw_deg,\n",
    "# p,q,r,u,v,w,tx,ty,tz,Thrust\n",
    "\n",
    "required_cols = ['Time_s','Height_m','X_m','Y_m','Roll_deg','Pitch_deg','Yaw_deg',\n",
    "                 'p','q','r','u','v','w','tx','ty','tz','Thrust']\n",
    "\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column {c} in data\")\n",
    "    \n",
    "# Inputs: time only\n",
    "t_np = safe_array(data['Time_s'])\n",
    "\n",
    "# Targets: we will predict these outputs (14)\n",
    "# We'll follow the sign convention used earlier: model z = -Height_m\n",
    "z_np = safe_array(data['Height_m'], negate=True)\n",
    "phi_np = safe_array(data['Roll_deg'], deg_to_rad=True)\n",
    "theta_np = safe_array(data['Pitch_deg'], deg_to_rad=True)\n",
    "psi_np = safe_array(data['Yaw_deg'], deg_to_rad=True)\n",
    "\n",
    "p_np = safe_array(data['p'])\n",
    "q_np = safe_array(data['q'])\n",
    "r_np = safe_array(data['r'])\n",
    "\n",
    "u_np = safe_array(data['u'])\n",
    "v_np = safe_array(data['v'])\n",
    "w_np = safe_array(data['w'])\n",
    "\n",
    "tx_np = safe_array(data['tx'])\n",
    "ty_np = safe_array(data['ty'])\n",
    "tz_np = safe_array(data['tz'])\n",
    "T_np  = safe_array(data['Thrust'])\n",
    "\n",
    "# Stack targets in a consistent order\n",
    "Y_np = np.hstack([z_np, phi_np, theta_np, psi_np,\n",
    "                  p_np, q_np, r_np,\n",
    "                  u_np, v_np, w_np,\n",
    "                  tx_np, ty_np, tz_np, T_np])  # shape (N,14)\n",
    "\n",
    "\n",
    "split_idx = int(0.7 * len(t_np))\n",
    "t_train, t_test = t_np[:split_idx], t_np[split_idx:]\n",
    "Y_train, Y_test = Y_np[:split_idx], Y_np[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(t_train)}\")\n",
    "print(f\"Test samples:  {len(t_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "\n",
    "# Convert to numpy if tensors\n",
    "if isinstance(t_train, torch.Tensor):\n",
    "    t_train = t_train.detach().cpu().numpy()\n",
    "if isinstance(Y_train, torch.Tensor):\n",
    "    Y_train = Y_train.detach().cpu().numpy()\n",
    "if isinstance(t_test, torch.Tensor):\n",
    "    t_test = t_test.detach().cpu().numpy()\n",
    "if isinstance(Y_test, torch.Tensor):\n",
    "    Y_test = Y_test.detach().cpu().numpy()\n",
    "\n",
    "# Fit only on training data\n",
    "t_scaler.fit(t_train)\n",
    "Y_scaler.fit(Y_train)\n",
    "\n",
    "# Transform both train and test\n",
    "t_train_scaled = t_scaler.transform(t_train)\n",
    "t_test_scaled = t_scaler.transform(t_test)\n",
    "Y_train_scaled = Y_scaler.transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 10 | Test batches: 5\n"
     ]
    }
   ],
   "source": [
    "def to_tensor_safe(x, device=\"cuda\"):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.clone().detach().to(torch.float32).to(device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type for tensor conversion: {type(x)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "t_train_tensor = to_tensor_safe(t_train_scaled, DEVICE)\n",
    "Y_train_tensor = to_tensor_safe(Y_train_scaled, DEVICE)\n",
    "t_test_tensor  = to_tensor_safe(t_test_scaled, DEVICE)\n",
    "Y_test_tensor  = to_tensor_safe(Y_test_scaled, DEVICE)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(t_train_tensor, Y_train_tensor)\n",
    "test_dataset  = TensorDataset(t_test_tensor,  Y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Define PINN model\n",
    "# -------------------------\n",
    "class PINNNet(nn.Module):\n",
    "    def __init__(self, hidden=256, layers=4, in_dim=1, out_dim=14):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + [hidden]*layers + [out_dim]\n",
    "        net = []\n",
    "        for i in range(len(dims)-1):\n",
    "            net.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims)-2:\n",
    "                net.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*net)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Ensure correct input shape\n",
    "        if t.ndim == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        return self.net(t)\n",
    "\n",
    "\n",
    "# Instantiate model, optimizer, and loss\n",
    "model = PINNNet(hidden=256, layers=4, in_dim=1, out_dim=Y_train_tensor.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_residuals(model, t_batch, Y_scaler, t_scaler, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Compute physics-based residuals in *physical units*.\n",
    "    t_batch: scaled time tensor with requires_grad=True (or must be set by caller)\n",
    "    \"\"\"\n",
    "    # Ensure correct device and grad flag (caller should set requires_grad_)\n",
    "    t_batch = t_batch.clone().detach().to(device).requires_grad_(True)\n",
    "\n",
    "    # Forward pass: scaled outputs (keeps graph)\n",
    "    pred_scaled = model(t_batch)  # (B,14)\n",
    "\n",
    "    # Compute d(y_scaled)/d(t_scaled) for each output via autograd\n",
    "    grads_scaled = []\n",
    "    for i in range(pred_scaled.shape[1]):\n",
    "        g = torch.autograd.grad(pred_scaled[:, i:i+1], t_batch,\n",
    "                                grad_outputs=torch.ones_like(pred_scaled[:, i:i+1]),\n",
    "                                retain_graph=True, create_graph=True)[0]\n",
    "        grads_scaled.append(g)\n",
    "    grads_scaled = torch.cat(grads_scaled, dim=1)  # (B,14)\n",
    "\n",
    "    # Chain rule: map scaled derivatives to physical derivatives\n",
    "    scale_t = float(t_scaler.scale_[0])\n",
    "    scale_y = torch.tensor(Y_scaler.scale_, dtype=torch.float32, device=device)\n",
    "    dy_dt_phys = grads_scaled * (scale_y / scale_t)\n",
    "\n",
    "    # Unscale predicted outputs to physical units\n",
    "    pred_phys = pred_scaled * scale_y + torch.tensor(Y_scaler.mean_, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Split outputs and derivatives\n",
    "    z, phi, theta, psi, p, q, r, u, v, w, tx, ty, tz, Th = torch.split(pred_phys, 1, dim=1)\n",
    "    z_t, phi_t, theta_t, psi_t, p_t, q_t, r_t, u_t, v_t, w_t, *_ = torch.split(dy_dt_phys, 1, dim=1)\n",
    "\n",
    "    # Physical constants\n",
    "    g_const = 9.81\n",
    "    m = 0.068\n",
    "    Jxx, Jyy, Jzz = 6.86e-5, 9.2e-5, 1.366e-4\n",
    "    t1, t2, t3 = (Jyy - Jzz) / Jxx, (Jzz - Jxx) / Jyy, (Jxx - Jyy) / Jzz\n",
    "\n",
    "    # Residual definitions (physical units)\n",
    "    f_z = z_t + (-torch.sin(theta)*u + torch.cos(theta)*torch.sin(phi)*v + torch.cos(theta)*torch.cos(phi)*w)\n",
    "    f_u = u_t - ( r*v - q*w - g_const*torch.sin(theta) - 0.1*u )\n",
    "    f_v = v_t - ( p*w - r*u + g_const*torch.cos(theta)*torch.sin(phi) - 0.1*v )\n",
    "    f_w = w_t - ( q*u - p*v + g_const*torch.cos(theta)*torch.cos(phi) - 0.1*w + (-Th)/m )\n",
    "\n",
    "    f_p = p_t - ( t1 * q * r + tx / Jxx - 2.0 * p )\n",
    "    f_q = q_t - ( t2 * p * r + ty / Jyy - 2.0 * q )\n",
    "    f_r = r_t - ( t3 * p * q + tz / Jzz - 2.0 * r )\n",
    "\n",
    "    f_phi = phi_t - ( p + torch.sin(phi)*torch.tan(theta)*q + torch.cos(phi)*torch.tan(theta)*r )\n",
    "    f_theta = theta_t - ( torch.cos(phi)*q - torch.sin(phi)*r )\n",
    "    f_psi = psi_t - ( torch.sin(phi)/torch.cos(theta)*q + torch.cos(phi)/torch.cos(theta)*r )\n",
    "\n",
    "    return {\n",
    "        'f_z': f_z,\n",
    "        'f_trans': torch.cat([f_u, f_v, f_w], dim=1),\n",
    "        'f_rot': torch.cat([f_p, f_q, f_r], dim=1),\n",
    "        'f_kin': torch.cat([f_phi, f_theta, f_psi], dim=1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8000 | train_total=9.322726e-01 | train_data=9.322726e-01 | train_phys=2.987708e+02 | val_total=1.321252e+00 | val_data=1.321252e+00 | val_phys=2.247320e+02 | lambda=0.00e+00 | elapsed=0.08 min\n",
      "Epoch 200/8000 | train_total=8.777571e-01 | train_data=8.777571e-01 | train_phys=1.852512e+02 | val_total=1.318595e+00 | val_data=1.318595e+00 | val_phys=1.828982e+02 | lambda=0.00e+00 | elapsed=17.67 min\n",
      "Epoch 400/8000 | train_total=8.730793e-01 | train_data=8.730793e-01 | train_phys=1.776133e+02 | val_total=1.328512e+00 | val_data=1.328512e+00 | val_phys=2.446159e+02 | lambda=0.00e+00 | elapsed=30.97 min\n",
      "Epoch 600/8000 | train_total=8.704816e-01 | train_data=8.704816e-01 | train_phys=1.843094e+02 | val_total=1.336181e+00 | val_data=1.336181e+00 | val_phys=1.650399e+02 | lambda=0.00e+00 | elapsed=42.92 min\n",
      "Epoch 800/8000 | train_total=8.733274e-01 | train_data=8.733274e-01 | train_phys=1.928491e+02 | val_total=1.335295e+00 | val_data=1.335295e+00 | val_phys=1.932756e+02 | lambda=0.00e+00 | elapsed=59.29 min\n",
      "Epoch 1000/8000 | train_total=8.743321e-01 | train_data=8.743321e-01 | train_phys=1.790129e+02 | val_total=1.329208e+00 | val_data=1.329208e+00 | val_phys=1.841317e+02 | lambda=0.00e+00 | elapsed=72.31 min\n",
      "Epoch 1200/8000 | train_total=8.713683e-01 | train_data=8.713683e-01 | train_phys=1.562631e+02 | val_total=1.333656e+00 | val_data=1.333656e+00 | val_phys=1.669946e+02 | lambda=0.00e+00 | elapsed=86.15 min\n",
      "Epoch 1400/8000 | train_total=8.715546e-01 | train_data=8.715546e-01 | train_phys=1.703810e+02 | val_total=1.334221e+00 | val_data=1.334221e+00 | val_phys=1.851951e+02 | lambda=0.00e+00 | elapsed=100.33 min\n",
      "Epoch 1600/8000 | train_total=8.661093e-01 | train_data=8.661093e-01 | train_phys=1.521712e+02 | val_total=1.329463e+00 | val_data=1.329463e+00 | val_phys=1.710969e+02 | lambda=0.00e+00 | elapsed=112.72 min\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "phys_losses = []\n",
    "data_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# quick safety\n",
    "assert len(train_loader) > 0, \"train_loader is empty\"\n",
    "assert len(test_loader) > 0, \"test_loader is empty\"\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_data_loss = 0.0\n",
    "    epoch_phys_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    # Physics lambda schedule\n",
    "    if epoch <= PRETRAIN_EPOCHS:\n",
    "        lambda_phys = 0.0\n",
    "    elif epoch <= PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS:\n",
    "        frac = (epoch - PRETRAIN_EPOCHS) / PHYS_RAMP_EPOCHS\n",
    "        lambda_phys = LAMBDA_BASE * frac\n",
    "    else:\n",
    "        lambda_phys = LAMBDA_BASE\n",
    "\n",
    "    # Training loop (batches)\n",
    "    for t_batch, y_batch in train_loader:\n",
    "        n_batches += 1\n",
    "        t_batch = t_batch.clone().detach().to(DEVICE).requires_grad_(True)   # need grad for physics\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_scaled = model(t_batch)\n",
    "\n",
    "        # Data loss\n",
    "        loss_data = criterion(pred_scaled, y_batch)\n",
    "\n",
    "        # Physics residuals (requires autograd)\n",
    "        residuals = physics_residuals(model, t_batch, Y_scaler, t_scaler, DEVICE)\n",
    "        loss_z = torch.mean(residuals['f_z']**2)\n",
    "        loss_trans = torch.mean(residuals['f_trans']**2)\n",
    "        loss_rot = torch.mean(residuals['f_rot']**2)\n",
    "        loss_kin = torch.mean(residuals['f_kin']**2)\n",
    "        loss_phys = L_z*loss_z + L_trans*loss_trans + L_rot*loss_rot + L_kin*loss_kin\n",
    "\n",
    "        # Combine and step\n",
    "        loss_total = loss_data + lambda_phys * loss_phys\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_data_loss += loss_data.item()\n",
    "        epoch_phys_loss += loss_phys.item()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    epoch_data_loss = epoch_data_loss / n_batches if n_batches > 0 else float('nan')\n",
    "    epoch_phys_loss = epoch_phys_loss / n_batches if n_batches > 0 else float('nan')\n",
    "    train_losses.append(epoch_data_loss + lambda_phys * epoch_phys_loss)\n",
    "    data_losses.append(epoch_data_loss)\n",
    "    phys_losses.append(epoch_phys_loss)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Validation / Test evaluation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_data_loss = 0.0\n",
    "    val_phys_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    for t_val, y_val in test_loader:\n",
    "        val_batches += 1\n",
    "        # Move to device (no grad yet)\n",
    "        t_val = t_val.clone().detach().to(DEVICE)\n",
    "        y_val = y_val.to(DEVICE)\n",
    "\n",
    "        # ---- data loss: compute without grad for speed ----\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(t_val)\n",
    "            loss_val_data = criterion(pred_val, y_val)\n",
    "            val_data_loss += loss_val_data.item()\n",
    "\n",
    "        # ---- physics loss: need gradients for time-derivatives ----\n",
    "        # create a gradient-enabled copy of t_val and compute residuals\n",
    "        t_val_grad = t_val.clone().detach().requires_grad_(True)\n",
    "        residuals_val = physics_residuals(model, t_val_grad, Y_scaler, t_scaler, DEVICE)\n",
    "        loss_val_phys = (L_z*torch.mean(residuals_val['f_z']**2) +\n",
    "                         L_trans*torch.mean(residuals_val['f_trans']**2) +\n",
    "                         L_rot*torch.mean(residuals_val['f_rot']**2) +\n",
    "                         L_kin*torch.mean(residuals_val['f_kin']**2))\n",
    "        # .item() is safe here because loss_val_phys is scalar tensor\n",
    "        val_phys_loss += loss_val_phys.item()\n",
    "\n",
    "    # Normalize validation losses\n",
    "    val_data_loss = val_data_loss / val_batches if val_batches > 0 else float('nan')\n",
    "    val_phys_loss = val_phys_loss / val_batches if val_batches > 0 else float('nan')\n",
    "    val_losses.append(val_data_loss + lambda_phys * val_phys_loss)\n",
    "\n",
    "    # Logging\n",
    "    if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "              f\"train_total={train_losses[-1]:.6e} | train_data={epoch_data_loss:.6e} | train_phys={epoch_phys_loss:.6e} | \"\n",
    "              f\"val_total={val_losses[-1]:.6e} | val_data={val_data_loss:.6e} | val_phys={val_phys_loss:.6e} | \"\n",
    "              f\"lambda={lambda_phys:.2e} | elapsed={elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7) Save model and scalers\n",
    "# -------------------------\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    't_scaler_mean': t_scaler.mean_,\n",
    "    't_scaler_scale': t_scaler.scale_,\n",
    "    'Y_scaler_mean': Y_scaler.mean_,\n",
    "    'Y_scaler_scale': Y_scaler.scale_\n",
    "}, 'pinn_quad_model.pth')\n",
    "print(\"Saved model to pinn_quad_model.pth\")\n",
    "\n",
    "# -------------------------\n",
    "# 8) Diagnostics plots\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='total (data + lambda*phys)')\n",
    "plt.plot(data_losses, label='data loss')\n",
    "plt.plot(phys_losses, label='phys loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs (binned)')\n",
    "plt.title('Training losses (log scale)')\n",
    "plt.show()\n",
    "\n",
    "# Quick parity plot for a subset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # pick first 2000 points for parity check\n",
    "    Ncheck = min(2000, t_tensor.shape[0])\n",
    "    t_check = t_tensor[:Ncheck].clone().to(DEVICE).requires_grad_(False)\n",
    "    pred_scaled_check = model(t_check)\n",
    "    pred_phys_check = pred_scaled_check.cpu().numpy() * Y_scaler.scale_ + Y_scaler.mean_\n",
    "    true_phys = Y_np[:Ncheck,:]\n",
    "\n",
    "fig, axs = plt.subplots(4,1, figsize=(8,10))\n",
    "axs[0].plot(true_phys[:,0], label='z_true'); axs[0].plot(pred_phys_check[:,0], label='z_pred'); axs[0].legend()\n",
    "axs[1].plot(true_phys[:,1]*180/np.pi, label='phi_true'); axs[1].plot(pred_phys_check[:,1]*180/np.pi, label='phi_pred'); axs[1].legend()\n",
    "axs[2].plot(true_phys[:,2]*180/np.pi, label='theta_true'); axs[2].plot(pred_phys_check[:,2]*180/np.pi, label='theta_pred'); axs[2].legend()\n",
    "axs[3].plot(true_phys[:,3]*180/np.pi, label='psi_true'); axs[3].plot(pred_phys_check[:,3]*180/np.pi, label='psi_pred'); axs[3].legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

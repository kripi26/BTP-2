{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data_generation\"               \n",
    "FILE_PATTERN = \"quad_data_run_*.csv\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3 \n",
    "PRETRAIN_EPOCHS = 2000       # train data-only first\n",
    "PHYS_RAMP_EPOCHS = 2000      # ramp physics weight gradually\n",
    "TOTAL_EPOCHS = PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS + 4000\n",
    "PRINT_EVERY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics weights (base values; physics weight will be ramped)\n",
    "# Increased LAMBDA_BASE to give physics a stronger voice.\n",
    "LAMBDA_BASE = 1e-2\n",
    "\n",
    "# Manually balanced weights to counteract imbalanced loss magnitudes.\n",
    "# Rotational loss is likely the largest, so we scale the others up.\n",
    "L_z = 500.0\n",
    "L_trans = 100.0\n",
    "L_rot = 1.0  # Baseline\n",
    "L_kin = 500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrotor constants (same as MATLAB)\n",
    "Jxx = 6.86e-5; Jyy = 9.2e-5; Jzz = 1.366e-4\n",
    "m = 0.068; g = 9.81\n",
    "t1 = (Jyy - Jzz) / Jxx\n",
    "t2 = (Jzz - Jxx) / Jyy\n",
    "t3 = (Jxx - Jyy) / Jzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_array(values, deg_to_rad=False, negate=False):\n",
    "    arr = np.array(values, dtype=np.float64)\n",
    "    \n",
    "    arr[np.isnan(arr)] = 0.0\n",
    "    arr[np.isinf(arr)] = 0.0\n",
    "\n",
    "    if deg_to_rad:\n",
    "        arr = arr * np.pi / 180.0\n",
    "    if negate:\n",
    "        arr = -arr\n",
    "\n",
    "    arr = np.clip(arr, -1e6, 1e6)\n",
    "\n",
    "    return arr.reshape(-1, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 files, total samples = 16000\n"
     ]
    }
   ],
   "source": [
    "csv_files = sorted(glob.glob(os.path.join(DATA_DIR, FILE_PATTERN)))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found with pattern {FILE_PATTERN} in {DATA_DIR}\")\n",
    "\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(csv_files)} files, total samples = {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No NaN values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "nan_counts = data.isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(nan_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing NaN values:\")\n",
    "    print(nan_cols)\n",
    "    print(\"\\nRows with NaN values:\")\n",
    "    print(data[data.isna().any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No NaN values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No Inf or -Inf values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "inf_mask = np.isinf(data.select_dtypes(include=[np.number])).any()\n",
    "inf_cols = inf_mask[inf_mask].index.tolist()\n",
    "\n",
    "if len(inf_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing Inf or -Inf values:\")\n",
    "    print(inf_cols)\n",
    "    print(\"\\nRows with Inf values:\")\n",
    "    print(data[np.isinf(data.select_dtypes(include=[np.number])).any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No Inf or -Inf values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 11200\n",
      "Test samples:  4800\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Select columns + preprocessing\n",
    "# -------------------------\n",
    "# Column names as generated by MATLAB code earlier\n",
    "# Time_s,Height_m,X_m,Y_m,Roll_deg,Pitch_deg,Yaw_deg,\n",
    "# p,q,r,u,v,w,tx,ty,tz,Thrust\n",
    "\n",
    "required_cols = ['Time_s','Height_m','X_m','Y_m','Roll_deg','Pitch_deg','Yaw_deg',\n",
    "                 'p','q','r','u','v','w','tx','ty','tz','Thrust']\n",
    "\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column {c} in data\")\n",
    "    \n",
    "# Inputs: time only\n",
    "t_np = safe_array(data['Time_s'])\n",
    "\n",
    "# Targets: we will predict these outputs (14)\n",
    "# We'll follow the sign convention used earlier: model z = -Height_m\n",
    "z_np = safe_array(data['Height_m'], negate=True)\n",
    "phi_np = safe_array(data['Roll_deg'], deg_to_rad=True)\n",
    "theta_np = safe_array(data['Pitch_deg'], deg_to_rad=True)\n",
    "psi_np = safe_array(data['Yaw_deg'], deg_to_rad=True)\n",
    "\n",
    "p_np = safe_array(data['p'])\n",
    "q_np = safe_array(data['q'])\n",
    "r_np = safe_array(data['r'])\n",
    "\n",
    "u_np = safe_array(data['u'])\n",
    "v_np = safe_array(data['v'])\n",
    "w_np = safe_array(data['w'])\n",
    "\n",
    "tx_np = safe_array(data['tx'])\n",
    "ty_np = safe_array(data['ty'])\n",
    "tz_np = safe_array(data['tz'])\n",
    "T_np  = safe_array(data['Thrust'])\n",
    "\n",
    "# Stack targets in a consistent order\n",
    "Y_np = np.hstack([z_np, phi_np, theta_np, psi_np,\n",
    "                  p_np, q_np, r_np,\n",
    "                  u_np, v_np, w_np,\n",
    "                  tx_np, ty_np, tz_np, T_np])  # shape (N,14)\n",
    "\n",
    "\n",
    "split_idx = int(0.7 * len(t_np))\n",
    "t_train, t_test = t_np[:split_idx], t_np[split_idx:]\n",
    "Y_train, Y_test = Y_np[:split_idx], Y_np[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(t_train)}\")\n",
    "print(f\"Test samples:  {len(t_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "\n",
    "# Convert to numpy if tensors\n",
    "if isinstance(t_train, torch.Tensor):\n",
    "    t_train = t_train.detach().cpu().numpy()\n",
    "if isinstance(Y_train, torch.Tensor):\n",
    "    Y_train = Y_train.detach().cpu().numpy()\n",
    "if isinstance(t_test, torch.Tensor):\n",
    "    t_test = t_test.detach().cpu().numpy()\n",
    "if isinstance(Y_test, torch.Tensor):\n",
    "    Y_test = Y_test.detach().cpu().numpy()\n",
    "\n",
    "# Fit only on training data\n",
    "t_scaler.fit(t_train)\n",
    "Y_scaler.fit(Y_train)\n",
    "\n",
    "# Transform both train and test\n",
    "t_train_scaled = t_scaler.transform(t_train)\n",
    "t_test_scaled = t_scaler.transform(t_test)\n",
    "Y_train_scaled = Y_scaler.transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 10 | Test batches: 5\n"
     ]
    }
   ],
   "source": [
    "def to_tensor_safe(x, device=\"cuda\"):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.clone().detach().to(torch.float32).to(device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type for tensor conversion: {type(x)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "t_train_tensor = to_tensor_safe(t_train_scaled, DEVICE)\n",
    "Y_train_tensor = to_tensor_safe(Y_train_scaled, DEVICE)\n",
    "t_test_tensor  = to_tensor_safe(t_test_scaled, DEVICE)\n",
    "Y_test_tensor  = to_tensor_safe(Y_test_scaled, DEVICE)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(t_train_tensor, Y_train_tensor)\n",
    "test_dataset  = TensorDataset(t_test_tensor,  Y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Define PINN model\n",
    "# -------------------------\n",
    "class PINNNet(nn.Module):\n",
    "    def __init__(self, hidden=256, layers=4, in_dim=1, out_dim=14):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + [hidden]*layers + [out_dim]\n",
    "        net = []\n",
    "        for i in range(len(dims)-1):\n",
    "            net.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims)-2:\n",
    "                net.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*net)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Ensure correct input shape\n",
    "        if t.ndim == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        return self.net(t)\n",
    "\n",
    "\n",
    "# # Instantiate model, optimizer, and loss\n",
    "# model = PINNNet(hidden=256, layers=4, in_dim=1, out_dim=Y_train_tensor.shape[1]).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.7)\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_residuals(model, t_batch, Y_scaler, t_scaler, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Compute physics-based residuals in *physical units*.\n",
    "    t_batch: scaled time tensor with requires_grad=True (or must be set by caller)\n",
    "    \"\"\"\n",
    "    # Ensure correct device and grad flag (caller should set requires_grad_)\n",
    "    t_batch = t_batch.clone().detach().to(device).requires_grad_(True)\n",
    "\n",
    "    # Forward pass: scaled outputs (keeps graph)\n",
    "    pred_scaled = model(t_batch)  # (B,14)\n",
    "\n",
    "    # Compute d(y_scaled)/d(t_scaled) for each output via autograd\n",
    "    grads_scaled = []\n",
    "    for i in range(pred_scaled.shape[1]):\n",
    "        g = torch.autograd.grad(pred_scaled[:, i:i+1], t_batch,\n",
    "                                grad_outputs=torch.ones_like(pred_scaled[:, i:i+1]),\n",
    "                                retain_graph=True, create_graph=True)[0]\n",
    "        grads_scaled.append(g)\n",
    "    grads_scaled = torch.cat(grads_scaled, dim=1)  # (B,14)\n",
    "\n",
    "    # Chain rule: map scaled derivatives to physical derivatives\n",
    "    scale_t = float(t_scaler.scale_[0])\n",
    "    scale_y = torch.tensor(Y_scaler.scale_, dtype=torch.float32, device=device)\n",
    "    dy_dt_phys = grads_scaled * (scale_y / scale_t)\n",
    "\n",
    "    # Unscale predicted outputs to physical units\n",
    "    pred_phys = pred_scaled * scale_y + torch.tensor(Y_scaler.mean_, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Split outputs and derivatives\n",
    "    z, phi, theta, psi, p, q, r, u, v, w, tx, ty, tz, Th = torch.split(pred_phys, 1, dim=1)\n",
    "    z_t, phi_t, theta_t, psi_t, p_t, q_t, r_t, u_t, v_t, w_t, *_ = torch.split(dy_dt_phys, 1, dim=1)\n",
    "\n",
    "    # Physical constants\n",
    "    g_const = 9.81\n",
    "    m = 0.068\n",
    "    Jxx, Jyy, Jzz = 6.86e-5, 9.2e-5, 1.366e-4\n",
    "    t1, t2, t3 = (Jyy - Jzz) / Jxx, (Jzz - Jxx) / Jyy, (Jxx - Jyy) / Jzz\n",
    "\n",
    "    # Residual definitions (physical units)\n",
    "    f_z = z_t + (-torch.sin(theta)*u + torch.cos(theta)*torch.sin(phi)*v + torch.cos(theta)*torch.cos(phi)*w)\n",
    "    f_u = u_t - ( r*v - q*w - g_const*torch.sin(theta) - 0.1*u )\n",
    "    f_v = v_t - ( p*w - r*u + g_const*torch.cos(theta)*torch.sin(phi) - 0.1*v )\n",
    "    f_w = w_t - ( q*u - p*v + g_const*torch.cos(theta)*torch.cos(phi) - 0.1*w + (-Th)/m )\n",
    "\n",
    "    f_p = p_t - ( t1 * q * r + tx / Jxx - 2.0 * p )\n",
    "    f_q = q_t - ( t2 * p * r + ty / Jyy - 2.0 * q )\n",
    "    f_r = r_t - ( t3 * p * q + tz / Jzz - 2.0 * r )\n",
    "\n",
    "    f_phi = phi_t - ( p + torch.sin(phi)*torch.tan(theta)*q + torch.cos(phi)*torch.tan(theta)*r )\n",
    "    f_theta = theta_t - ( torch.cos(phi)*q - torch.sin(phi)*r )\n",
    "    f_psi = psi_t - ( torch.sin(phi)/torch.cos(theta)*q + torch.cos(phi)/torch.cos(theta)*r )\n",
    "\n",
    "    return {\n",
    "        'f_z': f_z,\n",
    "        'f_trans': torch.cat([f_u, f_v, f_w], dim=1),\n",
    "        'f_rot': torch.cat([f_p, f_q, f_r], dim=1),\n",
    "        'f_kin': torch.cat([f_phi, f_theta, f_psi], dim=1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training process...\n",
      "Stage 1: Adam optimizer for 4000 epochs.\n",
      "Stage 2: L-BFGS optimizer for 2000 epochs.\n",
      "Epoch 1/6000 | train_total=9.492976e-01 | train_data=9.492976e-01 | train_phys=1.085683e+05 | val_total=1.340528e+00 | val_data=1.340528e+00 | val_phys=4.613396e+04 | lambda=0.00e+00 | elapsed=0.04 min\n",
      "Epoch 200/6000 | train_total=8.737006e-01 | train_data=8.737006e-01 | train_phys=5.878815e+04 | val_total=1.329365e+00 | val_data=1.329365e+00 | val_phys=5.452993e+04 | lambda=0.00e+00 | elapsed=6.20 min\n",
      "Epoch 400/6000 | train_total=8.718980e-01 | train_data=8.718980e-01 | train_phys=5.708983e+04 | val_total=1.333683e+00 | val_data=1.333683e+00 | val_phys=5.272342e+04 | lambda=0.00e+00 | elapsed=12.62 min\n",
      "Epoch 600/6000 | train_total=8.747392e-01 | train_data=8.747392e-01 | train_phys=7.055830e+04 | val_total=1.327671e+00 | val_data=1.327671e+00 | val_phys=7.568796e+04 | lambda=0.00e+00 | elapsed=19.16 min\n",
      "Epoch 800/6000 | train_total=8.687876e-01 | train_data=8.687876e-01 | train_phys=7.048680e+04 | val_total=1.319918e+00 | val_data=1.319918e+00 | val_phys=8.274803e+04 | lambda=0.00e+00 | elapsed=25.57 min\n",
      "Epoch 1000/6000 | train_total=8.691652e-01 | train_data=8.691652e-01 | train_phys=6.106235e+04 | val_total=1.337634e+00 | val_data=1.337634e+00 | val_phys=6.296350e+04 | lambda=0.00e+00 | elapsed=32.09 min\n",
      "Epoch 1200/6000 | train_total=8.708446e-01 | train_data=8.708446e-01 | train_phys=6.148217e+04 | val_total=1.334922e+00 | val_data=1.334922e+00 | val_phys=5.633200e+04 | lambda=0.00e+00 | elapsed=43.83 min\n"
     ]
    }
   ],
   "source": [
    "# --- Optimizer and Scheduler Setup ---\n",
    "model = PINNNet(hidden=256, layers=4, in_dim=1, out_dim=Y_train_tensor.shape[1]).to(DEVICE)\n",
    "# We will use Adam first, then switch to LBFGS\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_adam, step_size=1000, gamma=0.7)\n",
    "# LBFGS has a different signature and is better for fine-tuning\n",
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "    model.parameters(), \n",
    "    lr=0.1,  # L-BFGS often works well with a larger initial learning rate\n",
    "    max_iter=20, \n",
    "    history_size=100,\n",
    "    line_search_fn=\"strong_wolfe\"\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# --- Training Configuration ---\n",
    "ADAM_EPOCHS = 4000  # Epochs to run with Adam optimizer\n",
    "LBFGS_EPOCHS = 2000 # Epochs for L-BFGS fine-tuning\n",
    "TOTAL_EPOCHS = ADAM_EPOCHS + LBFGS_EPOCHS\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "data_losses, phys_losses = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Safety checks\n",
    "assert len(train_loader) > 0, \"train_loader is empty\"\n",
    "assert len(test_loader) > 0, \"test_loader is empty\"\n",
    "\n",
    "print(\"🚀 Starting training process...\")\n",
    "print(f\"Stage 1: Adam optimizer for {ADAM_EPOCHS} epochs.\")\n",
    "print(f\"Stage 2: L-BFGS optimizer for {LBFGS_EPOCHS} epochs.\")\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    \n",
    "    # --- Lambda Schedule for Physics Loss ---\n",
    "    if epoch <= PRETRAIN_EPOCHS:\n",
    "        lambda_phys = 0.0\n",
    "    elif epoch <= PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS:\n",
    "        frac = (epoch - PRETRAIN_EPOCHS) / PHYS_RAMP_EPOCHS\n",
    "        lambda_phys = LAMBDA_BASE * frac\n",
    "    else:\n",
    "        lambda_phys = LAMBDA_BASE\n",
    "        \n",
    "    epoch_data_loss = 0.0\n",
    "    epoch_phys_loss = 0.0\n",
    "    \n",
    "    # --- Define the L-BFGS Closure ---\n",
    "    # This function is called repeatedly by the L-BFGS optimizer\n",
    "    def closure():\n",
    "        # Important: clear gradients inside the closure\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "        \n",
    "        # We need to re-evaluate the loss on the entire training set for L-BFGS\n",
    "        t_batch_full = t_train_tensor.clone().detach().requires_grad_(True)\n",
    "        y_batch_full = Y_train_tensor.clone().detach()\n",
    "\n",
    "        # Data loss\n",
    "        pred_scaled = model(t_batch_full)\n",
    "        loss_data = criterion(pred_scaled, y_batch_full)\n",
    "        \n",
    "        # Physics loss\n",
    "        residuals = physics_residuals(model, t_batch_full, Y_scaler, t_scaler, DEVICE)\n",
    "        loss_z = torch.mean(residuals['f_z']**2)\n",
    "        loss_trans = torch.mean(residuals['f_trans']**2)\n",
    "        loss_rot = torch.mean(residuals['f_rot']**2)\n",
    "        loss_kin = torch.mean(residuals['f_kin']**2)\n",
    "        loss_phys = L_z*loss_z + L_trans*loss_trans + L_rot*loss_rot + L_kin*loss_kin\n",
    "\n",
    "        # Total loss\n",
    "        loss_total = loss_data + lambda_phys * loss_phys\n",
    "        loss_total.backward()\n",
    "        \n",
    "        # Store loss values for logging (optional, use .item() to detach)\n",
    "        global epoch_data_loss_lbfgs, epoch_phys_loss_lbfgs\n",
    "        epoch_data_loss_lbfgs = loss_data.item()\n",
    "        epoch_phys_loss_lbfgs = loss_phys.item()\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    # --- Optimizer Switching Logic ---\n",
    "    if epoch <= ADAM_EPOCHS:\n",
    "        # ADAM OPTIMIZATION (iterates over batches)\n",
    "        n_batches = 0\n",
    "        for t_batch, y_batch in train_loader:\n",
    "            n_batches += 1\n",
    "            t_batch = t_batch.clone().detach().requires_grad_(True)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer_adam.zero_grad()\n",
    "            pred_scaled = model(t_batch)\n",
    "            loss_data = criterion(pred_scaled, y_batch)\n",
    "            \n",
    "            residuals = physics_residuals(model, t_batch, Y_scaler, t_scaler, DEVICE)\n",
    "            loss_z = torch.mean(residuals['f_z']**2)\n",
    "            loss_trans = torch.mean(residuals['f_trans']**2)\n",
    "            loss_rot = torch.mean(residuals['f_rot']**2)\n",
    "            loss_kin = torch.mean(residuals['f_kin']**2)\n",
    "            loss_phys = L_z*loss_z + L_trans*loss_trans + L_rot*loss_rot + L_kin*loss_kin\n",
    "            \n",
    "            loss_total = loss_data + lambda_phys * loss_phys\n",
    "            loss_total.backward()\n",
    "            optimizer_adam.step()\n",
    "            \n",
    "            epoch_data_loss += loss_data.item()\n",
    "            epoch_phys_loss += loss_phys.item()\n",
    "        \n",
    "        scheduler.step() # Step the LR scheduler\n",
    "        epoch_data_loss /= n_batches\n",
    "        epoch_phys_loss /= n_batches\n",
    "    \n",
    "    else:\n",
    "        # L-BFGS OPTIMIZATION (uses the closure on the full dataset)\n",
    "        if epoch == ADAM_EPOCHS + 1:\n",
    "            print(\"\\n--- Switching to L-BFGS Optimizer for fine-tuning ---\\n\")\n",
    "        \n",
    "        # Define globals to be updated by closure\n",
    "        epoch_data_loss_lbfgs = 0.0\n",
    "        epoch_phys_loss_lbfgs = 0.0\n",
    "        \n",
    "        optimizer_lbfgs.step(closure)\n",
    "        \n",
    "        # Retrieve loss values from the globals updated in the closure\n",
    "        epoch_data_loss = epoch_data_loss_lbfgs\n",
    "        epoch_phys_loss = epoch_phys_loss_lbfgs\n",
    "\n",
    "    # --- Logging and Validation (same as before) ---\n",
    "    train_losses.append(epoch_data_loss + lambda_phys * epoch_phys_loss)\n",
    "    data_losses.append(epoch_data_loss)\n",
    "    phys_losses.append(epoch_phys_loss)\n",
    "    \n",
    "    # Validation logic remains unchanged...\n",
    "    model.eval()\n",
    "    val_data_loss, val_phys_loss, val_batches = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for t_val, y_val in test_loader:\n",
    "            val_batches += 1\n",
    "            t_val, y_val = t_val.to(DEVICE), y_val.to(DEVICE)\n",
    "            pred_val = model(t_val)\n",
    "            val_data_loss += criterion(pred_val, y_val).item()\n",
    "    \n",
    "    t_val_grad = t_test_tensor.clone().detach().requires_grad_(True)\n",
    "    residuals_val = physics_residuals(model, t_val_grad, Y_scaler, t_scaler, DEVICE)\n",
    "    loss_val_phys = (L_z*torch.mean(residuals_val['f_z']**2) +\n",
    "                     L_trans*torch.mean(residuals_val['f_trans']**2) +\n",
    "                     L_rot*torch.mean(residuals_val['f_rot']**2) +\n",
    "                     L_kin*torch.mean(residuals_val['f_kin']**2))\n",
    "    val_phys_loss = loss_val_phys.item() # No need to loop, do it on the whole test set\n",
    "    val_data_loss /= val_batches\n",
    "    val_losses.append(val_data_loss + lambda_phys * val_phys_loss)\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0 or epoch == 1 or epoch == ADAM_EPOCHS + 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "              f\"train_total={train_losses[-1]:.6e} | train_data={epoch_data_loss:.6e} | train_phys={epoch_phys_loss:.6e} | \"\n",
    "              f\"val_total={val_losses[-1]:.6e} | val_data={val_data_loss:.6e} | val_phys={val_phys_loss:.6e} | \"\n",
    "              f\"lambda={lambda_phys:.2e} | elapsed={elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting L-BFGS-Only Training Process...\n",
      "Total Epochs: 4000\n",
      "--------------------------------------------------\n",
      "Epoch 1/4000 | train_total=9.289064e-01 | train_data=9.289064e-01 | train_phys=2.092999e+02 | val_total=1.318430e+00 | val_data=1.318430e+00 | val_phys=1.998407e+02 | lambda=0.00e+00 | elapsed=0.86 min\n",
      "Epoch 200/4000 | train_total=8.692654e-01 | train_data=8.692654e-01 | train_phys=1.761588e+02 | val_total=1.332838e+00 | val_data=1.332838e+00 | val_phys=1.835271e+02 | lambda=0.00e+00 | elapsed=74.32 min\n",
      "Epoch 400/4000 | train_total=8.692654e-01 | train_data=8.692654e-01 | train_phys=1.761588e+02 | val_total=1.332838e+00 | val_data=1.332838e+00 | val_phys=1.835271e+02 | lambda=0.00e+00 | elapsed=102.38 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m epoch_data_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     75\u001b[0m epoch_phys_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 76\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# --- Logging and Validation ---\u001b[39;00m\n\u001b[0;32m     79\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_data_loss \u001b[38;5;241m+\u001b[39m lambda_phys \u001b[38;5;241m*\u001b[39m epoch_phys_loss)\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:330\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    327\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[0;32m    332\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 55\u001b[0m, in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m loss_data \u001b[38;5;241m=\u001b[39m criterion(pred_scaled, y_train_full)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# --- Physics Loss (using the balanced weights) ---\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m residuals \u001b[38;5;241m=\u001b[39m \u001b[43mphysics_residuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m loss_z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residuals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_z\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     57\u001b[0m loss_trans \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residuals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_trans\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 15\u001b[0m, in \u001b[0;36mphysics_residuals\u001b[1;34m(model, t_batch, Y_scaler, t_scaler, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m grads_scaled \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 15\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m     grads_scaled\u001b[38;5;241m.\u001b[39mappend(g)\n\u001b[0;32m     19\u001b[0m grads_scaled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(grads_scaled, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,14)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\parid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Optimizer Setup ---\n",
    "# We will use L-BFGS for the entire training duration.\n",
    "model = PINNNet(hidden=256, layers=4, in_dim=1, out_dim=Y_train_tensor.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.LBFGS(\n",
    "    model.parameters(), \n",
    "    lr=0.1,  # A learning rate of 0.1 is a good starting point for L-BFGS\n",
    "    max_iter=25, \n",
    "    history_size=100,\n",
    "    line_search_fn=\"strong_wolfe\" # Recommended for stability\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --- Training Configuration ---\n",
    "# L-BFGS epochs are more intensive, so we can use fewer than the Adam+LBFGS combo.\n",
    "TOTAL_EPOCHS = 4000\n",
    "train_losses, val_losses = [], []\n",
    "data_losses, phys_losses = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Safety checks\n",
    "assert len(train_loader) > 0, \"train_loader is empty\"\n",
    "assert len(test_loader) > 0, \"test_loader is empty\"\n",
    "\n",
    "print(\"🚀 Starting L-BFGS-Only Training Process...\")\n",
    "print(f\"Total Epochs: {TOTAL_EPOCHS}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# We need the full training data for the L-BFGS closure\n",
    "t_train_full = t_train_tensor.clone().detach().requires_grad_(True)\n",
    "y_train_full = Y_train_tensor.clone().detach()\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    \n",
    "    # --- Lambda Schedule for Physics Loss ---\n",
    "    if epoch <= PRETRAIN_EPOCHS:\n",
    "        lambda_phys = 0.0\n",
    "    elif epoch <= PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS:\n",
    "        frac = (epoch - PRETRAIN_EPOCHS) / PHYS_RAMP_EPOCHS\n",
    "        lambda_phys = LAMBDA_BASE * frac\n",
    "    else:\n",
    "        lambda_phys = LAMBDA_BASE\n",
    "        \n",
    "    # --- L-BFGS Closure Function ---\n",
    "    # This function is called repeatedly by the L-BFGS optimizer.\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Data Loss ---\n",
    "        pred_scaled = model(t_train_full)\n",
    "        loss_data = criterion(pred_scaled, y_train_full)\n",
    "        \n",
    "        # --- Physics Loss (using the balanced weights) ---\n",
    "        residuals = physics_residuals(model, t_train_full, Y_scaler, t_scaler, DEVICE)\n",
    "        loss_z = torch.mean(residuals['f_z']**2)\n",
    "        loss_trans = torch.mean(residuals['f_trans']**2)\n",
    "        loss_rot = torch.mean(residuals['f_rot']**2)\n",
    "        loss_kin = torch.mean(residuals['f_kin']**2)\n",
    "        loss_phys = L_z*loss_z + L_trans*loss_trans + L_rot*loss_rot + L_kin*loss_kin\n",
    "\n",
    "        # --- Total Loss ---\n",
    "        loss_total = loss_data + lambda_phys * loss_phys\n",
    "        loss_total.backward()\n",
    "        \n",
    "        # Make loss values accessible outside the closure\n",
    "        global epoch_data_loss, epoch_phys_loss\n",
    "        epoch_data_loss = loss_data.item()\n",
    "        epoch_phys_loss = loss_phys.item()\n",
    "        return loss_total\n",
    "\n",
    "    # --- Optimizer Step ---\n",
    "    # Define globals that will be updated by the closure\n",
    "    epoch_data_loss = 0.0\n",
    "    epoch_phys_loss = 0.0\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # --- Logging and Validation ---\n",
    "    train_losses.append(epoch_data_loss + lambda_phys * epoch_phys_loss)\n",
    "    data_losses.append(epoch_data_loss)\n",
    "    phys_losses.append(epoch_phys_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    val_data_loss, val_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for t_val, y_val in test_loader:\n",
    "            val_batches += 1\n",
    "            t_val, y_val = t_val.to(DEVICE), y_val.to(DEVICE)\n",
    "            pred_val = model(t_val)\n",
    "            val_data_loss += criterion(pred_val, y_val).item()\n",
    "            \n",
    "    t_val_grad = t_test_tensor.clone().detach().requires_grad_(True)\n",
    "    residuals_val = physics_residuals(model, t_val_grad, Y_scaler, t_scaler, DEVICE)\n",
    "    loss_val_phys = (L_z*torch.mean(residuals_val['f_z']**2) +\n",
    "                     L_trans*torch.mean(residuals_val['f_trans']**2) +\n",
    "                     L_rot*torch.mean(residuals_val['f_rot']**2) +\n",
    "                     L_kin*torch.mean(residuals_val['f_kin']**2))\n",
    "    val_phys_loss = loss_val_phys.item()\n",
    "    val_data_loss /= val_batches\n",
    "    val_losses.append(val_data_loss + lambda_phys * val_phys_loss)\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "              f\"train_total={train_losses[-1]:.6e} | train_data={epoch_data_loss:.6e} | train_phys={epoch_phys_loss:.6e} | \"\n",
    "              f\"val_total={val_losses[-1]:.6e} | val_data={val_data_loss:.6e} | val_phys={val_phys_loss:.6e} | \"\n",
    "              f\"lambda={lambda_phys:.2e} | elapsed={elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7) Save model and scalers\n",
    "# -------------------------\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    't_scaler_mean': t_scaler.mean_,\n",
    "    't_scaler_scale': t_scaler.scale_,\n",
    "    'Y_scaler_mean': Y_scaler.mean_,\n",
    "    'Y_scaler_scale': Y_scaler.scale_\n",
    "}, 'pinn_quad_model.pth')\n",
    "print(\"Saved model to pinn_quad_model.pth\")\n",
    "\n",
    "# -------------------------\n",
    "# 8) Diagnostics plots\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='total (data + lambda*phys)')\n",
    "plt.plot(data_losses, label='data loss')\n",
    "plt.plot(phys_losses, label='phys loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs (binned)')\n",
    "plt.title('Training losses (log scale)')\n",
    "plt.show()\n",
    "\n",
    "# Quick parity plot for a subset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # pick first 2000 points for parity check\n",
    "    Ncheck = min(2000, t_tensor.shape[0])\n",
    "    t_check = t_tensor[:Ncheck].clone().to(DEVICE).requires_grad_(False)\n",
    "    pred_scaled_check = model(t_check)\n",
    "    pred_phys_check = pred_scaled_check.cpu().numpy() * Y_scaler.scale_ + Y_scaler.mean_\n",
    "    true_phys = Y_np[:Ncheck,:]\n",
    "\n",
    "fig, axs = plt.subplots(4,1, figsize=(8,10))\n",
    "axs[0].plot(true_phys[:,0], label='z_true'); axs[0].plot(pred_phys_check[:,0], label='z_pred'); axs[0].legend()\n",
    "axs[1].plot(true_phys[:,1]*180/np.pi, label='phi_true'); axs[1].plot(pred_phys_check[:,1]*180/np.pi, label='phi_pred'); axs[1].legend()\n",
    "axs[2].plot(true_phys[:,2]*180/np.pi, label='theta_true'); axs[2].plot(pred_phys_check[:,2]*180/np.pi, label='theta_pred'); axs[2].legend()\n",
    "axs[3].plot(true_phys[:,3]*180/np.pi, label='psi_true'); axs[3].plot(pred_phys_check[:,3]*180/np.pi, label='psi_pred'); axs[3].legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

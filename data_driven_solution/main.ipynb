{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data_generation\"               \n",
    "FILE_PATTERN = \"quad_data_run_*.csv\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3 \n",
    "PRETRAIN_EPOCHS = 2000       # train data-only first\n",
    "PHYS_RAMP_EPOCHS = 2000      # ramp physics weight gradually\n",
    "TOTAL_EPOCHS = PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS + 4000\n",
    "PRINT_EVERY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics weights (base values; physics weight will be ramped)\n",
    "LAMBDA_BASE = 1e-4\n",
    "L_z = 1.0\n",
    "L_trans = 1.0\n",
    "L_rot = 1.0\n",
    "L_kin = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrotor constants (same as MATLAB)\n",
    "Jxx = 6.86e-5; Jyy = 9.2e-5; Jzz = 1.366e-4\n",
    "m = 0.068; g = 9.81\n",
    "t1 = (Jyy - Jzz) / Jxx\n",
    "t2 = (Jzz - Jxx) / Jyy\n",
    "t3 = (Jxx - Jyy) / Jzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_array(values, deg_to_rad=False, negate=False):\n",
    "    arr = np.array(values, dtype=np.float64)\n",
    "    \n",
    "    arr[np.isnan(arr)] = 0.0\n",
    "    arr[np.isinf(arr)] = 0.0\n",
    "\n",
    "    if deg_to_rad:\n",
    "        arr = arr * np.pi / 180.0\n",
    "    if negate:\n",
    "        arr = -arr\n",
    "\n",
    "    arr = np.clip(arr, -1e6, 1e6)\n",
    "\n",
    "    return arr.reshape(-1, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 files, total samples = 16000\n"
     ]
    }
   ],
   "source": [
    "csv_files = sorted(glob.glob(os.path.join(DATA_DIR, FILE_PATTERN)))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found with pattern {FILE_PATTERN} in {DATA_DIR}\")\n",
    "\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(csv_files)} files, total samples = {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No NaN values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "nan_counts = data.isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(nan_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing NaN values:\")\n",
    "    print(nan_cols)\n",
    "    print(\"\\nRows with NaN values:\")\n",
    "    print(data[data.isna().any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No NaN values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ No Inf or -Inf values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "inf_mask = np.isinf(data.select_dtypes(include=[np.number])).any()\n",
    "inf_cols = inf_mask[inf_mask].index.tolist()\n",
    "\n",
    "if len(inf_cols) > 0:\n",
    "    print(\"\\n⚠️ Columns containing Inf or -Inf values:\")\n",
    "    print(inf_cols)\n",
    "    print(\"\\nRows with Inf values:\")\n",
    "    print(data[np.isinf(data.select_dtypes(include=[np.number])).any(axis=1)])\n",
    "else:\n",
    "    print(\"\\n✅ No Inf or -Inf values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 11200\n",
      "Test samples:  4800\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Select columns + preprocessing\n",
    "# -------------------------\n",
    "# Column names as generated by MATLAB code earlier\n",
    "# Time_s,Height_m,X_m,Y_m,Roll_deg,Pitch_deg,Yaw_deg,\n",
    "# p,q,r,u,v,w,tx,ty,tz,Thrust\n",
    "\n",
    "required_cols = ['Time_s','Height_m','X_m','Y_m','Roll_deg','Pitch_deg','Yaw_deg',\n",
    "                 'p','q','r','u','v','w','tx','ty','tz','Thrust']\n",
    "\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column {c} in data\")\n",
    "    \n",
    "# Inputs: time only\n",
    "t_np = safe_array(data['Time_s'])\n",
    "\n",
    "# Targets: we will predict these outputs (14)\n",
    "# We'll follow the sign convention used earlier: model z = -Height_m\n",
    "z_np = safe_array(data['Height_m'], negate=True)\n",
    "phi_np = safe_array(data['Roll_deg'], deg_to_rad=True)\n",
    "theta_np = safe_array(data['Pitch_deg'], deg_to_rad=True)\n",
    "psi_np = safe_array(data['Yaw_deg'], deg_to_rad=True)\n",
    "\n",
    "p_np = safe_array(data['p'])\n",
    "q_np = safe_array(data['q'])\n",
    "r_np = safe_array(data['r'])\n",
    "\n",
    "u_np = safe_array(data['u'])\n",
    "v_np = safe_array(data['v'])\n",
    "w_np = safe_array(data['w'])\n",
    "\n",
    "tx_np = safe_array(data['tx'])\n",
    "ty_np = safe_array(data['ty'])\n",
    "tz_np = safe_array(data['tz'])\n",
    "T_np  = safe_array(data['Thrust'])\n",
    "\n",
    "# Stack targets in a consistent order\n",
    "Y_np = np.hstack([z_np, phi_np, theta_np, psi_np,\n",
    "                  p_np, q_np, r_np,\n",
    "                  u_np, v_np, w_np,\n",
    "                  tx_np, ty_np, tz_np, T_np])  # shape (N,14)\n",
    "\n",
    "\n",
    "split_idx = int(0.7 * len(t_np))\n",
    "t_train, t_test = t_np[:split_idx], t_np[split_idx:]\n",
    "Y_train, Y_test = Y_np[:split_idx], Y_np[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(t_train)}\")\n",
    "print(f\"Test samples:  {len(t_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "\n",
    "# Convert to numpy if tensors\n",
    "if isinstance(t_train, torch.Tensor):\n",
    "    t_train = t_train.detach().cpu().numpy()\n",
    "if isinstance(Y_train, torch.Tensor):\n",
    "    Y_train = Y_train.detach().cpu().numpy()\n",
    "if isinstance(t_test, torch.Tensor):\n",
    "    t_test = t_test.detach().cpu().numpy()\n",
    "if isinstance(Y_test, torch.Tensor):\n",
    "    Y_test = Y_test.detach().cpu().numpy()\n",
    "\n",
    "# Fit only on training data\n",
    "t_scaler.fit(t_train)\n",
    "Y_scaler.fit(Y_train)\n",
    "\n",
    "# Transform both train and test\n",
    "t_train_scaled = t_scaler.transform(t_train)\n",
    "t_test_scaled = t_scaler.transform(t_test)\n",
    "Y_train_scaled = Y_scaler.transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 10 | Test batches: 5\n"
     ]
    }
   ],
   "source": [
    "def to_tensor_safe(x, device=\"cuda\"):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.clone().detach().to(torch.float32).to(device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type for tensor conversion: {type(x)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "t_train_tensor = to_tensor_safe(t_train_scaled, DEVICE)\n",
    "Y_train_tensor = to_tensor_safe(Y_train_scaled, DEVICE)\n",
    "t_test_tensor  = to_tensor_safe(t_test_scaled, DEVICE)\n",
    "Y_test_tensor  = to_tensor_safe(Y_test_scaled, DEVICE)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(t_train_tensor, Y_train_tensor)\n",
    "test_dataset  = TensorDataset(t_test_tensor,  Y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Define PINN model\n",
    "# -------------------------\n",
    "class PINNNet(nn.Module):\n",
    "    def __init__(self, hidden=256, layers=4, in_dim=1, out_dim=14):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + [hidden]*layers + [out_dim]\n",
    "        net = []\n",
    "        for i in range(len(dims)-1):\n",
    "            net.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims)-2:\n",
    "                net.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*net)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Ensure correct input shape\n",
    "        if t.ndim == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        return self.net(t)\n",
    "\n",
    "\n",
    "# Instantiate model, optimizer, and loss\n",
    "model = PINNNet(hidden=256, layers=4, in_dim=1, out_dim=Y_train_tensor.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5) Physics residuals function\n",
    "# -------------------------\n",
    "\n",
    "def physics_residuals(model, t_batch, Y_scaler, t_scaler, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Compute physics-based residuals in *physical units*.\n",
    "    Handles scaled inputs and outputs, consistent with normalization.\n",
    "    \n",
    "    Parameters:\n",
    "        model: trained PINN model\n",
    "        t_batch: (B,1) scaled time tensor with requires_grad=True\n",
    "        Y_scaler, t_scaler: fitted StandardScaler objects\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        dict of residual tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure t_batch has gradient enabled\n",
    "    t_batch = t_batch.clone().detach().to(device).requires_grad_(True)\n",
    "\n",
    "    # Forward pass: scaled outputs\n",
    "    pred_scaled = model(t_batch)  # (B,14)\n",
    "    \n",
    "    # Compute derivatives of each output wrt time\n",
    "    grads_scaled = []\n",
    "    for i in range(pred_scaled.shape[1]):\n",
    "        g = torch.autograd.grad(pred_scaled[:, i:i+1], t_batch,\n",
    "                                grad_outputs=torch.ones_like(pred_scaled[:, i:i+1]),\n",
    "                                retain_graph=True, create_graph=True)[0]\n",
    "        grads_scaled.append(g)\n",
    "    grads_scaled = torch.cat(grads_scaled, dim=1)  # (B,14)\n",
    "\n",
    "    # Chain rule: map scaled derivatives to physical derivatives\n",
    "    scale_t = float(t_scaler.scale_[0])\n",
    "    scale_y = torch.tensor(Y_scaler.scale_, dtype=torch.float32, device=device)\n",
    "    dy_dt_phys = grads_scaled * (scale_y / scale_t)\n",
    "\n",
    "    # Unscale predicted outputs to physical units\n",
    "    pred_phys = pred_scaled * scale_y + torch.tensor(Y_scaler.mean_, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Split outputs\n",
    "    z, phi, theta, psi, p, q, r, u, v, w, tx, ty, tz, Th = torch.split(pred_phys, 1, dim=1)\n",
    "    z_t, phi_t, theta_t, psi_t, p_t, q_t, r_t, u_t, v_t, w_t, *_ = torch.split(dy_dt_phys, 1, dim=1)\n",
    "\n",
    "    # Physical constants (ensure they’re defined globally or passed in)\n",
    "    g_const = 9.81\n",
    "    m = 0.068\n",
    "    Jxx, Jyy, Jzz = 6.86e-5, 9.2e-5, 1.366e-4\n",
    "    t1, t2, t3 = (Jyy - Jzz) / Jxx, (Jzz - Jxx) / Jyy, (Jxx - Jyy) / Jzz\n",
    "\n",
    "    # ----------------------------\n",
    "    # Define residual equations\n",
    "    # ----------------------------\n",
    "    f_z = z_t + (-torch.sin(theta)*u + torch.cos(theta)*torch.sin(phi)*v + torch.cos(theta)*torch.cos(phi)*w)\n",
    "    f_u = u_t - (r*v - q*w - g_const*torch.sin(theta) - 0.1*u)\n",
    "    f_v = v_t - (p*w - r*u + g_const*torch.cos(theta)*torch.sin(phi) - 0.1*v)\n",
    "    f_w = w_t - (q*u - p*v + g_const*torch.cos(theta)*torch.cos(phi) - 0.1*w + (-Th)/m)\n",
    "\n",
    "    f_p = p_t - (t1 * q * r + tx / Jxx - 2.0 * p)\n",
    "    f_q = q_t - (t2 * p * r + ty / Jyy - 2.0 * q)\n",
    "    f_r = r_t - (t3 * p * q + tz / Jzz - 2.0 * r)\n",
    "\n",
    "    f_phi = phi_t - (p + torch.sin(phi)*torch.tan(theta)*q + torch.cos(phi)*torch.tan(theta)*r)\n",
    "    f_theta = theta_t - (torch.cos(phi)*q - torch.sin(phi)*r)\n",
    "    f_psi = psi_t - (torch.sin(phi)/torch.cos(theta)*q + torch.cos(phi)/torch.cos(theta)*r)\n",
    "\n",
    "    return {\n",
    "        'f_z': f_z,\n",
    "        'f_trans': torch.cat([f_u, f_v, f_w], dim=1),\n",
    "        'f_rot': torch.cat([f_p, f_q, f_r], dim=1),\n",
    "        'f_kin': torch.cat([f_phi, f_theta, f_psi], dim=1)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m model(t_val)\n\u001b[0;32m     80\u001b[0m loss_val_data \u001b[38;5;241m=\u001b[39m criterion(pred_val, y_val)\n\u001b[1;32m---> 81\u001b[0m residuals_val \u001b[38;5;241m=\u001b[39m \u001b[43mphysics_residuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m loss_val_phys \u001b[38;5;241m=\u001b[39m (L_z\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(residuals_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_z\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     83\u001b[0m                  L_trans\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(residuals_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_trans\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     84\u001b[0m                  L_rot\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(residuals_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_rot\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     85\u001b[0m                  L_kin\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(residuals_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_kin\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     87\u001b[0m val_data_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_val_data\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mphysics_residuals\u001b[1;34m(model, t_batch, Y_scaler, t_scaler, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m grads_scaled \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 29\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m     grads_scaled\u001b[38;5;241m.\u001b[39mappend(g)\n\u001b[0;32m     33\u001b[0m grads_scaled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(grads_scaled, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,14)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Training loop with warm-start and physics ramp\n",
    "# -------------------------\n",
    "\n",
    "train_losses = []\n",
    "phys_losses = []\n",
    "data_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_data_loss = 0.0\n",
    "    epoch_phys_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Physics lambda schedule\n",
    "    # -----------------------------\n",
    "    if epoch <= PRETRAIN_EPOCHS:\n",
    "        lambda_phys = 0.0\n",
    "    elif epoch <= PRETRAIN_EPOCHS + PHYS_RAMP_EPOCHS:\n",
    "        frac = (epoch - PRETRAIN_EPOCHS) / PHYS_RAMP_EPOCHS\n",
    "        lambda_phys = LAMBDA_BASE * frac\n",
    "    else:\n",
    "        lambda_phys = LAMBDA_BASE\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training over batches\n",
    "    # -----------------------------\n",
    "    for t_batch, y_batch in train_loader:   # <-- use train_loader\n",
    "        n_batches += 1\n",
    "        t_batch = t_batch.clone().detach().to(DEVICE).requires_grad_(True)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_scaled = model(t_batch)\n",
    "\n",
    "        # Data loss (MSE)\n",
    "        loss_data = criterion(pred_scaled, y_batch)\n",
    "\n",
    "        # Physics residual loss\n",
    "        residuals = physics_residuals(model, t_batch, Y_scaler, t_scaler, DEVICE)\n",
    "        loss_z = torch.mean(residuals['f_z']**2)\n",
    "        loss_trans = torch.mean(residuals['f_trans']**2)\n",
    "        loss_rot = torch.mean(residuals['f_rot']**2)\n",
    "        loss_kin = torch.mean(residuals['f_kin']**2)\n",
    "        loss_phys = L_z*loss_z + L_trans*loss_trans + L_rot*loss_rot + L_kin*loss_kin\n",
    "\n",
    "        # Total loss\n",
    "        loss_total = loss_data + lambda_phys * loss_phys\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_data_loss += loss_data.item()\n",
    "        epoch_phys_loss += loss_phys.item()\n",
    "\n",
    "    # Average over batches\n",
    "    epoch_data_loss /= n_batches\n",
    "    epoch_phys_loss /= n_batches\n",
    "    train_losses.append(epoch_data_loss + lambda_phys*epoch_phys_loss)\n",
    "    data_losses.append(epoch_data_loss)\n",
    "    phys_losses.append(epoch_phys_loss)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Optional: validation/test evaluation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_data_loss = 0.0\n",
    "    val_phys_loss = 0.0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for t_val, y_val in test_loader:\n",
    "            val_batches += 1\n",
    "            t_val = t_val.to(DEVICE)\n",
    "            y_val = y_val.to(DEVICE)\n",
    "            pred_val = model(t_val)\n",
    "\n",
    "            loss_val_data = criterion(pred_val, y_val)\n",
    "            residuals_val = physics_residuals(model, t_val, Y_scaler, t_scaler, DEVICE)\n",
    "            loss_val_phys = (L_z*torch.mean(residuals_val['f_z']**2) +\n",
    "                             L_trans*torch.mean(residuals_val['f_trans']**2) +\n",
    "                             L_rot*torch.mean(residuals_val['f_rot']**2) +\n",
    "                             L_kin*torch.mean(residuals_val['f_kin']**2))\n",
    "\n",
    "            val_data_loss += loss_val_data.item()\n",
    "            val_phys_loss += loss_val_phys.item()\n",
    "\n",
    "    val_data_loss /= val_batches\n",
    "    val_phys_loss /= val_batches\n",
    "    val_losses.append(val_data_loss + lambda_phys*val_phys_loss)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Logging\n",
    "    # -----------------------------\n",
    "    if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "              f\"train_total={train_losses[-1]:.6e} | train_data={epoch_data_loss:.6e} | train_phys={epoch_phys_loss:.6e} | \"\n",
    "              f\"val_total={val_losses[-1]:.6e} | val_data={val_data_loss:.6e} | val_phys={val_phys_loss:.6e} | \"\n",
    "              f\"lambda={lambda_phys:.2e} | elapsed={elapsed/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7) Save model and scalers\n",
    "# -------------------------\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    't_scaler_mean': t_scaler.mean_,\n",
    "    't_scaler_scale': t_scaler.scale_,\n",
    "    'Y_scaler_mean': Y_scaler.mean_,\n",
    "    'Y_scaler_scale': Y_scaler.scale_\n",
    "}, 'pinn_quad_model.pth')\n",
    "print(\"Saved model to pinn_quad_model.pth\")\n",
    "\n",
    "# -------------------------\n",
    "# 8) Diagnostics plots\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='total (data + lambda*phys)')\n",
    "plt.plot(data_losses, label='data loss')\n",
    "plt.plot(phys_losses, label='phys loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs (binned)')\n",
    "plt.title('Training losses (log scale)')\n",
    "plt.show()\n",
    "\n",
    "# Quick parity plot for a subset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # pick first 2000 points for parity check\n",
    "    Ncheck = min(2000, t_tensor.shape[0])\n",
    "    t_check = t_tensor[:Ncheck].clone().to(DEVICE).requires_grad_(False)\n",
    "    pred_scaled_check = model(t_check)\n",
    "    pred_phys_check = pred_scaled_check.cpu().numpy() * Y_scaler.scale_ + Y_scaler.mean_\n",
    "    true_phys = Y_np[:Ncheck,:]\n",
    "\n",
    "fig, axs = plt.subplots(4,1, figsize=(8,10))\n",
    "axs[0].plot(true_phys[:,0], label='z_true'); axs[0].plot(pred_phys_check[:,0], label='z_pred'); axs[0].legend()\n",
    "axs[1].plot(true_phys[:,1]*180/np.pi, label='phi_true'); axs[1].plot(pred_phys_check[:,1]*180/np.pi, label='phi_pred'); axs[1].legend()\n",
    "axs[2].plot(true_phys[:,2]*180/np.pi, label='theta_true'); axs[2].plot(pred_phys_check[:,2]*180/np.pi, label='theta_pred'); axs[2].legend()\n",
    "axs[3].plot(true_phys[:,3]*180/np.pi, label='psi_true'); axs[3].plot(pred_phys_check[:,3]*180/np.pi, label='psi_pred'); axs[3].legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
